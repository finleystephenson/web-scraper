---
phase: 02-scraping
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [app.py, templates/index.html]
autonomous: true
---

<objective>
Add scraping logic to extract SEO vitals and contact information from URLs.

Purpose: Transform placeholder responses into actual scraped data, completing core functionality.
Output: Working scraper that extracts title, H1, meta description, emails, phones, and social links.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-01-SUMMARY.md

@app.py
@templates/index.html
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add scraping functions to app.py</name>
  <files>app.py</files>
  <action>
Add scraping logic to app.py:

1. Import requests and BeautifulSoup at top:
   ```python
   import requests
   from bs4 import BeautifulSoup
   import re
   ```

2. Create `scrape_url(url)` function that:
   - Fetches URL with requests.get() (timeout=10, handle exceptions)
   - Parses HTML with BeautifulSoup
   - Extracts SEO vitals:
     - title: soup.title.string if soup.title else None
     - h1: soup.find('h1').get_text(strip=True) if soup.find('h1') else None
     - meta_description: soup.find('meta', attrs={'name': 'description'})['content'] if exists
   - Extracts contact info:
     - emails: from href="mailto:" links + regex r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' in page text
     - phones: from href="tel:" links + regex for phone patterns
     - socials: links containing linkedin.com, twitter.com, x.com, facebook.com, instagram.com
   - Returns dict with all extracted data (or error message if fetch fails)

3. Update /scan route to:
   - Call scrape_url(url)
   - Pass result dict to template

Handle errors gracefully - if request fails, return error message to display.
  </action>
  <verify>python -c "from app import app, scrape_url; print('Import OK')" succeeds</verify>
  <done>scrape_url function extracts SEO vitals and contact info, /scan route uses it</done>
</task>

<task type="auto">
  <name>Task 2: Update template to display scraped results</name>
  <files>templates/index.html</files>
  <action>
Update templates/index.html to display actual scraped data:

1. SEO Vitals section - replace "Pending (Phase 2)" with actual values:
   - Page Title: {{ result.title or 'Not found' }}
   - H1 Tag: {{ result.h1 or 'Not found' }}
   - Meta Description: {{ result.meta_description or 'Not found' }}

2. Contact Information section:
   - If result.emails: list each email as clickable mailto link
   - If result.phones: list each phone as clickable tel link
   - If result.socials: list each social link with platform name
   - If no contact info at all: show "No contact info detected"

3. Error handling:
   - If result.error exists, show error message in red alert box
   - Example: "Failed to fetch URL: Connection timeout"

Use Tailwind classes for styling:
- Links: text-blue-600 hover:underline
- Errors: bg-red-50 border border-red-200 text-red-700 rounded-lg p-4
- Contact list items: flex items-center gap-2 with appropriate icons or labels
  </action>
  <verify>flask run, submit URL, see actual scraped data (not placeholders)</verify>
  <done>Template displays real SEO vitals and contact info from scraping</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from app import app, scrape_url"` imports without error
- [ ] Submit a real URL (e.g., https://example.com) and see actual title/H1
- [ ] Submit URL with contact info and see emails/phones/socials extracted
- [ ] Submit invalid URL and see graceful error message
</verification>

<success_criteria>
- All tasks completed
- Scraping returns real data for valid URLs
- Contact extraction finds emails, phones, social links
- Error handling shows user-friendly messages
- No Python errors or import issues
</success_criteria>

<output>
After completion, create `.planning/phases/02-scraping/02-01-SUMMARY.md`
</output>
